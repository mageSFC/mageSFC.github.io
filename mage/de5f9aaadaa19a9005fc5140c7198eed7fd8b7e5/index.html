<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Hadoop学习笔记总结 | 马哥私房菜</title><meta name="author" content="mage"><meta name="copyright" content="mage"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="引言Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop学习笔记总结">
<meta property="og:url" content="https://magesfc.github.io/mage/de5f9aaadaa19a9005fc5140c7198eed7fd8b7e5/">
<meta property="og:site_name" content="马哥私房菜">
<meta property="og:description" content="引言Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://t.mwm.moe/fj/">
<meta property="article:published_time" content="2022-02-23T09:47:19.000Z">
<meta property="article:modified_time" content="2022-02-23T09:47:19.000Z">
<meta property="article:author" content="mage">
<meta property="article:tag" content="hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://t.mwm.moe/fj/"><link rel="shortcut icon" href="https://www.zeekrlife.com/favicon.png"><link rel="canonical" href="https://magesfc.github.io/mage/de5f9aaadaa19a9005fc5140c7198eed7fd8b7e5/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: mage","link":"链接: ","source":"来源: 马哥私房菜","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop学习笔记总结',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-02-23 17:47:19'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><style type="text/css">.card-announcement .social-button{margin:.6rem 0 0 0;text-align:center}.card-announcement .social-button a{display:block;background-color:var(--btn-bg);color:var(--btn-color);text-align:center;line-height:2.4;margin:4px 0}.card-announcement .social-button a:hover{background-color:var(--btn-hover-color)}</style><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">213</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">228</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">40</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 精选文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/21cfbf15/"><span> 🚀 快速开始</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/dc584b87/"><span> 📑 主题页面</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/4aa8abbe/"><span> 🛠 主题配置-1</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/ceeb73f/"><span> 🛠 主题配置-2</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/98d20436/"><span> ❓ 主题问答</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/4073eda/"><span> ⚡️ 进阶教程</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/198a4240/"><span> ✨ 更新日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://butterfly.js.org/link/"><i class="fa-fw fas fa-thumbs-up"></i><span> 其他示例</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://t.mwm.moe/fj/')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">马哥私房菜</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 精选文档</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/21cfbf15/"><span> 🚀 快速开始</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/dc584b87/"><span> 📑 主题页面</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/4aa8abbe/"><span> 🛠 主题配置-1</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/ceeb73f/"><span> 🛠 主题配置-2</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/98d20436/"><span> ❓ 主题问答</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/4073eda/"><span> ⚡️ 进阶教程</span></a></li><li><a class="site-page child" target="_blank" rel="noopener" href="https://butterfly.js.org/posts/198a4240/"><span> ✨ 更新日志</span></a></li></ul></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://butterfly.js.org/link/"><i class="fa-fw fas fa-thumbs-up"></i><span> 其他示例</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop学习笔记总结</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-02-23T09:47:19.000Z" title="发表于 2022-02-23 17:47:19">2022-02-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-02-23T09:47:19.000Z" title="更新于 2022-02-23 17:47:19">2022-02-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/hadoop/">hadoop</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop学习笔记总结"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。<br>它和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。<br>HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。<br>HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。<br>HDFS是Apache Hadoop Core项目的一部分。这个项目的地址是<a target="_blank" rel="noopener" href="http://hadoop.apache.org/core/%E3%80%82">http://hadoop.apache.org/core/。</a></p>
<h1 id="Namenode-和-Datanode"><a href="#Namenode-和-Datanode" class="headerlink" title="Namenode 和 Datanode"></a>Namenode 和 Datanode</h1><p>HDFS采用master&#x2F;slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。<br>Namenode是一个中心服务器，负责管理文件系统的名字空间(namespace)以及客户端对文件的访问。<br>集群中的Datanode一般是一个节点一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的名字空间，<br>用户能够以文件的形式在上面存储数据。从内部看，一个文件其实被分成一个或多个数据块，这些块存储在<br>一组Datanode上。Namenode执行文件系统的名字空间操作，比如打开、关闭、重命名文件或目录。它也负责<br>确定数据块到具体Datanode节点的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一<br>调度下进行数据块的创建、删除和复制。</p>
<p>Namenode和Datanode被设计成可以在普通的商用机器上运行。这些机器一般运行着GNU&#x2F;Linux操作系统(OS)。<br>HDFS采用Java语言开发，因此任何支持Java的机器都可以部署Namenode或Datanode。由于采用了可移植性极<br>强的Java语言，使得HDFS可以部署到多种类型的机器上。一个典型的部署场景是一台机器上只运行一个Namenode实例，<br>而集群中的其它机器分别运行一个Datanode实例。这种架构并不排斥在一台机器上运行多个Datanode，<br>只不过这样的情况比较少见。</p>
<p>集群中单一Namenode的结构大大简化了系统的架构。Namenode是所有HDFS元数据的仲裁者和管理者，<br>这样，用户数据永远不会流过Namenode。</p>
<p>Hadoop部署模式有：本地模式、伪分布模式、完全分布式模式、HA完全分布式模式。</p>
<p>区分的依据是NameNode、DataNode、ResourceManager、NodeManager等模块运行在几个JVM进程、几个机器。</p>
<h1 id="安装hadoop准备"><a href="#安装hadoop准备" class="headerlink" title="安装hadoop准备"></a>安装hadoop准备</h1><p>1.创建hadoop系统账号</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里创建一个普通的linux系统账号，让hadoop运行在这个账号下面，不要直接运行在root账号下面</span></span><br><span class="line"><span class="comment"># 账号的home目录可以根据需要指定，这里默认使用/home/hadoop</span></span><br><span class="line"></span><br><span class="line">sudo useradd -m -s /bin/bash hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也不要设置密码，切换就用 su 来切换</span></span><br><span class="line">sudo su - hadoop</span><br></pre></td></tr></table></figure>

<p>2.安装 JAVA</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">java -version</span><br><span class="line">openjdk version <span class="string">&quot;1.8.0_91&quot;</span></span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_91-b14)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.91-b14, mixed mode)</span><br></pre></td></tr></table></figure>

<p>3.安装 SSH</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这个基本上是系统都自带的有了</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>4.生成ssh key</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">&#x27;hadoop-master&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置免密码的ssh登录</span></span><br><span class="line"><span class="built_in">cat</span> .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="built_in">chmod</span> 0600 .ssh/authorized_keys</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>5.下载 hadoop-2.9.1.tar.gz</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有个345M 大小</span></span><br><span class="line">http://mirrors.shu.edu.cn/apache/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#解压</span></span><br><span class="line">tar xvzf hadoop-2.9.1.tar.gz</span><br><span class="line"></span><br><span class="line">[hadoop@localhost ~]$ <span class="built_in">ls</span></span><br><span class="line">hadoop-2.9.1  hadoop-2.9.1.tar.gz</span><br><span class="line"></span><br><span class="line">[hadoop@localhost ~]$ <span class="built_in">ls</span> hadoop-2.9.1</span><br><span class="line">bin  etc  include  lib  libexec  LICENSE.txt  NOTICE.txt  README.txt  sbin  share</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@localhost ~]$ ll hadoop-2.9.1</span><br><span class="line">总用量 152</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 4月  16 2018 bin</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 4月  16 2018 etc</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 4月  16 2018 include</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 4月  16 2018 lib</span><br><span class="line">drwxr-xr-x. 2 hadoop hadoop   4096 4月  16 2018 libexec</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop 106210 4月  16 2018 LICENSE.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop  15915 4月  16 2018 NOTICE.txt</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop   1366 4月  16 2018 README.txt</span><br><span class="line">drwxr-xr-x. 3 hadoop hadoop   4096 4月  16 2018 sbin</span><br><span class="line">drwxr-xr-x. 4 hadoop hadoop   4096 4月  16 2018 share</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载到/home/hadoop家目录下面就可以了，解压也放到这个目录下面</span></span><br></pre></td></tr></table></figure>


<p>6.启动hadoop</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">├─java,28743 -Dproc_namenode -Xmx1000m -Djava.library.path=/home/hadoop/hadoop-2.9.1/lib -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop-hadoop-namenode-localhost.localdomain.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">├─java,28994 -Dproc_datanode -Xmx1000m -Djava.library.path=/home/hadoop/hadoop-2.9.1/lib -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop-hadoop-datanode-localhost.localdomain.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">├─java,29350 -Dproc_secondarynamenode -Xmx1000m -Djava.library.path=/home/hadoop/hadoop-2.9.1/lib -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop-hadoop-secondarynamenode-localhost.localdomain.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</span><br><span class="line"></span><br><span class="line">├─java,29606 -Dproc_resourcemanager -Xmx1000m -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=yarn-hadoop-resourcemanager-localhost.localdomain.log -Dyarn.log.file=yarn-hadoop-resourcemanager-localhost.localdomain.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=yarn-hadoop-resourcemanager-localhost.localdomain.log -Dyarn.log.file=yarn-hadoop-resourcemanager-localhost.localdomain.log -Dyarn.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -classpath /home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/common/*:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/*:/home/hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.9.1/etc/hadoop/rm-config/log4j.properties:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/timelineservice/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/timelineservice/lib/* org.apache.hadoop.yarn.server.resourcemanager.ResourceManager</span><br><span class="line"></span><br><span class="line">├─java,29930 -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-localhost.localdomain.log -Dyarn.log.file=yarn-hadoop-nodemanager-localhost.localdomain.log -Dyarn.home.dir= -Dyarn.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dyarn.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=yarn-hadoop-nodemanager-localhost.localdomain.log -Dyarn.log.file=yarn-hadoop-nodemanager-localhost.localdomain.log -Dyarn.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -classpath /home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/etc/hadoop:/home/hadoop/hadoop-2.9.1/share/hadoop/common/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/common/*:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/hdfs/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/*:/home/hadoop/hadoop-2.9.1/share/hadoop/mapreduce/lib/*:/home/hadoop/hadoop-2.9.1/share/hadoop/mapreduce/*:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.9.1/contrib/capacity-scheduler/*.jar:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/lib/*:/home/hadoop/hadoop-2.9.1/etc/hadoop/nm-config/log4j.properties:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/timelineservice/*:/home/hadoop/hadoop-2.9.1/share/hadoop/yarn/timelineservice/lib/* org.apache.hadoop.yarn.server.nodemanager.NodeManager</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="本地模式部署"><a href="#本地模式部署" class="headerlink" title="本地模式部署"></a>本地模式部署</h1><p>本地模式是最简单的模式，所有模块都运行与一个JVM进程中，使用的本地文件系统，而不是HDFS，本地模式主要是用于本地开发过程中的运行调试用。下载hadoop安装包后不用任何设置，默认的就是本地模式。</p>
<p>运行MapReduce程序，验证</p>
<p>1、 准备mapreduce输入文件wc.input</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> <span class="variable">$HOME</span>/wc.input</span><br><span class="line">hadoop mapreduce hive</span><br><span class="line">hbase spark storm</span><br><span class="line">sqoop hadoop hive</span><br><span class="line">spark hadoop</span><br></pre></td></tr></table></figure>

<p>2、 运行hadoop自带的mapreduce Demo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost ~]$ hadoop jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar wordcount wc.input output2</span><br><span class="line">18/10/22 12:39:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">18/10/22 12:39:26 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id</span><br><span class="line">18/10/22 12:39:26 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=</span><br><span class="line">18/10/22 12:39:26 INFO input.FileInputFormat: Total input files to process : 1</span><br><span class="line">18/10/22 12:39:26 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">18/10/22 12:39:27 INFO mapreduce.JobSubmitter: Submitting tokens <span class="keyword">for</span> job: job_local841462251_0001</span><br><span class="line">18/10/22 12:39:27 INFO mapreduce.Job: The url to track the job: http://localhost:8080/</span><br><span class="line">18/10/22 12:39:27 INFO mapreduce.Job: Running job: job_local841462251_0001</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: OutputCommitter <span class="built_in">set</span> <span class="keyword">in</span> config null</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:<span class="literal">false</span>, ignore cleanup failures: <span class="literal">false</span></span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: Waiting <span class="keyword">for</span> map tasks</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: Starting task: attempt_local841462251_0001_m_000000_0</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:<span class="literal">false</span>, ignore cleanup failures: <span class="literal">false</span></span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: Processing <span class="built_in">split</span>: file:/home/hadoop/wc.input:0+71</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: soft <span class="built_in">limit</span> at 83886080</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: kvstart = 26214396; length = 6553600</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask<span class="variable">$MapOutputBuffer</span></span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: </span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: Starting flush of map output</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: Spilling map output</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: bufstart = 0; bufend = 115; bufvoid = 104857600</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600</span><br><span class="line">18/10/22 12:39:27 INFO mapred.MapTask: Finished spill 0</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task: Task:attempt_local841462251_0001_m_000000_0 is <span class="keyword">done</span>. And is <span class="keyword">in</span> the process of committing</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: map</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task: Task <span class="string">&#x27;attempt_local841462251_0001_m_000000_0&#x27;</span> <span class="keyword">done</span>.</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local841462251_0001_m_000000_0</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: map task executor complete.</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: Waiting <span class="keyword">for</span> reduce tasks</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: Starting task: attempt_local841462251_0001_r_000000_0</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:<span class="literal">false</span>, ignore cleanup failures: <span class="literal">false</span></span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</span><br><span class="line">18/10/22 12:39:27 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@65deffe9</span><br><span class="line">18/10/22 12:39:27 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10</span><br><span class="line">18/10/22 12:39:27 INFO reduce.EventFetcher: attempt_local841462251_0001_r_000000_0 Thread started: EventFetcher <span class="keyword">for</span> fetching Map Completion Events</span><br><span class="line">18/10/22 12:39:27 INFO reduce.LocalFetcher: localfetcher<span class="comment">#1 about to shuffle output of map attempt_local841462251_0001_m_000000_0 decomp: 90 len: 94 to MEMORY</span></span><br><span class="line">18/10/22 12:39:27 INFO reduce.InMemoryMapOutput: Read 90 bytes from map-output <span class="keyword">for</span> attempt_local841462251_0001_m_000000_0</span><br><span class="line">18/10/22 12:39:27 INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 90, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;90</span><br><span class="line">18/10/22 12:39:27 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: 1 / 1 copied.</span><br><span class="line">18/10/22 12:39:27 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Merger: Merging 1 sorted segments</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81 bytes</span><br><span class="line">18/10/22 12:39:27 INFO reduce.MergeManagerImpl: Merged 1 segments, 90 bytes to disk to satisfy reduce memory <span class="built_in">limit</span></span><br><span class="line">18/10/22 12:39:27 INFO reduce.MergeManagerImpl: Merging 1 files, 94 bytes from disk</span><br><span class="line">18/10/22 12:39:27 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Merger: Merging 1 sorted segments</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81 bytes</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: 1 / 1 copied.</span><br><span class="line">18/10/22 12:39:27 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task: Task:attempt_local841462251_0001_r_000000_0 is <span class="keyword">done</span>. And is <span class="keyword">in</span> the process of committing</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: 1 / 1 copied.</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task: Task attempt_local841462251_0001_r_000000_0 is allowed to commit now</span><br><span class="line">18/10/22 12:39:27 INFO output.FileOutputCommitter: Saved output of task <span class="string">&#x27;attempt_local841462251_0001_r_000000_0&#x27;</span> to file:/home/hadoop/output2/_temporary/0/task_local841462251_0001_r_000000</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: reduce &gt; reduce</span><br><span class="line">18/10/22 12:39:27 INFO mapred.Task: Task <span class="string">&#x27;attempt_local841462251_0001_r_000000_0&#x27;</span> <span class="keyword">done</span>.</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: Finishing task: attempt_local841462251_0001_r_000000_0</span><br><span class="line">18/10/22 12:39:27 INFO mapred.LocalJobRunner: reduce task executor complete.</span><br><span class="line">18/10/22 12:39:28 INFO mapreduce.Job: Job job_local841462251_0001 running <span class="keyword">in</span> uber mode : <span class="literal">false</span></span><br><span class="line">18/10/22 12:39:28 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">18/10/22 12:39:28 INFO mapreduce.Job: Job job_local841462251_0001 completed successfully</span><br><span class="line">18/10/22 12:39:28 INFO mapreduce.Job: Counters: 30</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes <span class="built_in">read</span>=607284</span><br><span class="line">		FILE: Number of bytes written=1537636</span><br><span class="line">		FILE: Number of <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of large <span class="built_in">read</span> operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=4</span><br><span class="line">		Map output records=11</span><br><span class="line">		Map output bytes=115</span><br><span class="line">		Map output materialized bytes=94</span><br><span class="line">		Input <span class="built_in">split</span> bytes=91</span><br><span class="line">		Combine input records=11</span><br><span class="line">		Combine output records=7</span><br><span class="line">		Reduce input <span class="built_in">groups</span>=7</span><br><span class="line">		Reduce shuffle bytes=94</span><br><span class="line">		Reduce input records=7</span><br><span class="line">		Reduce output records=7</span><br><span class="line">		Spilled Records=14</span><br><span class="line">		Shuffled Maps =1</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=1</span><br><span class="line">		GC time elapsed (ms)=0</span><br><span class="line">		Total committed heap usage (bytes)=525336576</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=71</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=72</span><br><span class="line">[hadoop@localhost ~]$ </span><br><span class="line"></span><br><span class="line">这里可以看到job ID中有<span class="built_in">local</span>字样，说明是运行在本地模式下的。</span><br></pre></td></tr></table></figure>

<p>3、 查看输出文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@localhost ~]$ <span class="built_in">ls</span> -alh output2/</span><br><span class="line">总用量 20K</span><br><span class="line">drwxrwxr-x. 2 hadoop hadoop 4.0K 10月 22 12:39 .</span><br><span class="line">drwx------. 8 hadoop hadoop 4.0K 10月 22 12:39 ..</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop   60 10月 22 12:39 part-r-00000</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop   12 10月 22 12:39 .part-r-00000.crc</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop    0 10月 22 12:39 _SUCCESS</span><br><span class="line">-rw-r--r--. 1 hadoop hadoop    8 10月 22 12:39 ._SUCCESS.crc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出目录中有_SUCCESS文件说明JOB运行成功，part-r-00000是输出结果文件。 </span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Hadoop伪分布式模式安装"><a href="#Hadoop伪分布式模式安装" class="headerlink" title="Hadoop伪分布式模式安装"></a>Hadoop伪分布式模式安装</h1><p>1、 配置Hadoop环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.91-2.b14.fc22.x86_64/jre</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/home/hadoop/hadoop-2.9.1</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>

<p>2、 配置 hadoop-env.sh、mapred-env.sh、yarn-env.sh文件的JAVA_HOME参数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3、 配置core-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop/core-site.xml</span><br><span class="line">（1） fs.defaultFS参数配置的是HDFS的地址。</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://10.0.63.48:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">（2） hadoop.tmp.dir配置的是Hadoop临时目录</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/hadoop-2.9.1/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">默认的hadoop.tmp.dir是/tmp/hadoop-<span class="variable">$&#123;user.name&#125;</span>,此时有个问题就是NameNode会将HDFS的元数据存储在这个/tmp目录下，</span><br><span class="line">如果操作系统重启了，系统会清空/tmp目录下的东西，导致NameNode元数据丢失，是个非常严重的问题，所有我们应该修改这个路径。</span><br></pre></td></tr></table></figure>

<p>4、 配置hdfs-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">dfs.replication配置的是HDFS存储时的备份数量，因为这里是伪分布式环境只有一个节点，所以这里设置为1。</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>5、 格式化HDFS </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop-2.9.1]$ hdfs</span><br><span class="line">Usage: hdfs [--config confdir] [--loglevel loglevel] COMMAND</span><br><span class="line">       <span class="built_in">where</span> COMMAND is one of:</span><br><span class="line">  dfs                  run a filesystem <span class="built_in">command</span> on the file systems supported <span class="keyword">in</span> Hadoop.</span><br><span class="line">  classpath            prints the classpath</span><br><span class="line">  namenode -format     format the DFS filesystem</span><br><span class="line">  secondarynamenode    run the DFS secondary namenode</span><br><span class="line">  namenode             run the DFS namenode</span><br><span class="line">  journalnode          run the DFS journalnode</span><br><span class="line">  zkfc                 run the ZK Failover Controller daemon</span><br><span class="line">  datanode             run a DFS datanode</span><br><span class="line">  debug                run a Debug Admin to execute HDFS debug commands</span><br><span class="line">  dfsadmin             run a DFS admin client</span><br><span class="line">  dfsrouter            run the DFS router</span><br><span class="line">  dfsrouteradmin       manage Router-based federation</span><br><span class="line">  haadmin              run a DFS HA admin client</span><br><span class="line">  fsck                 run a DFS filesystem checking utility</span><br><span class="line">  balancer             run a cluster balancing utility</span><br><span class="line">  jmxget               get JMX exported values from NameNode or DataNode.</span><br><span class="line">  mover                run a utility to move block replicas across</span><br><span class="line">                       storage types</span><br><span class="line">  oiv                  apply the offline fsimage viewer to an fsimage</span><br><span class="line">  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage</span><br><span class="line">  oev                  apply the offline edits viewer to an edits file</span><br><span class="line">  fetchdt              fetch a delegation token from the NameNode</span><br><span class="line">  getconf              get config values from configuration</span><br><span class="line">  <span class="built_in">groups</span>               get the <span class="built_in">groups</span> <span class="built_in">which</span> <span class="built_in">users</span> belong to</span><br><span class="line">  snapshotDiff         diff two snapshots of a directory or diff the</span><br><span class="line">                       current directory contents with a snapshot</span><br><span class="line">  lsSnapshottableDir   list all snapshottable <span class="built_in">dirs</span> owned by the current user</span><br><span class="line">						Use -<span class="built_in">help</span> to see options</span><br><span class="line">  portmap              run a portmap service</span><br><span class="line">  nfs3                 run an NFS version 3 gateway</span><br><span class="line">  cacheadmin           configure the HDFS cache</span><br><span class="line">  crypto               configure HDFS encryption zones</span><br><span class="line">  storagepolicies      list/get/set block storage policies</span><br><span class="line">  version              <span class="built_in">print</span> the version</span><br><span class="line"></span><br><span class="line">Most commands <span class="built_in">print</span> <span class="built_in">help</span> when invoked w/o parameters.</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@localhost hadoop-2.9.1]$ hdfs namenode -format</span><br><span class="line">18/10/22 13:40:46 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">STARTUP_MSG: Starting NameNode</span><br><span class="line">STARTUP_MSG:   host = localhost.localdomain/127.0.0.1</span><br><span class="line">STARTUP_MSG:   args = [-format]</span><br><span class="line">STARTUP_MSG:   version = 2.9.1</span><br><span class="line">STARTUP_MSG:   classpath = *.jar</span><br><span class="line">STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e; compiled by <span class="string">&#x27;root&#x27;</span> on 2018-04-16T09:33Z</span><br><span class="line">STARTUP_MSG:   java = 1.8.0_91</span><br><span class="line">************************************************************/</span><br><span class="line">18/10/22 13:40:46 INFO namenode.NameNode: registered UNIX signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line">18/10/22 13:40:46 INFO namenode.NameNode: createNameNode [-format]</span><br><span class="line">18/10/22 13:40:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Formatting using clusterid: CID-85451f7e-c811-4028-8eee-62d3202c00bc</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSEditLog: Edit logging is async:<span class="literal">true</span></span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: KeyProvider: null</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: fsLock is fair: <span class="literal">true</span></span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: <span class="literal">false</span></span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: supergroup          = supergroup</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: isPermissionEnabled = <span class="literal">true</span></span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: HA Enabled: <span class="literal">false</span></span><br><span class="line">18/10/22 13:40:47 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage <span class="built_in">set</span> to 0. Disabling file IO profiling</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=<span class="literal">true</span></span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is <span class="built_in">set</span> to 000:00:00:00.000</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: The block deletion will start around 2018 十月 22 13:40:47</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: Computing capacity <span class="keyword">for</span> map BlocksMap</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: capacity      = 2^21 = 2097152 entries</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=<span class="literal">false</span></span><br><span class="line">18/10/22 13:40:47 WARN conf.Configuration: No unit <span class="keyword">for</span> dfs.heartbeat.interval(3) assuming SECONDS</span><br><span class="line">18/10/22 13:40:47 WARN conf.Configuration: No unit <span class="keyword">for</span> dfs.namenode.safemode.extension(30000) assuming MILLISECONDS</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: defaultReplication         = 1</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: maxReplication             = 512</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: minReplication             = 1</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000</span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: encryptDataTransfer        = <span class="literal">false</span></span><br><span class="line">18/10/22 13:40:47 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: Append Enabled: <span class="literal">true</span></span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: Computing capacity <span class="keyword">for</span> map INodeMap</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: capacity      = 2^20 = 1048576 entries</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSDirectory: ACLs enabled? <span class="literal">false</span></span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSDirectory: XAttrs enabled? <span class="literal">true</span></span><br><span class="line">18/10/22 13:40:47 INFO namenode.NameNode: Caching file names occurring more than 10 <span class="built_in">times</span></span><br><span class="line">18/10/22 13:40:47 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: <span class="literal">false</span></span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: Computing capacity <span class="keyword">for</span> map cachedBlocks</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: capacity      = 2^18 = 262144 entries</span><br><span class="line">18/10/22 13:40:47 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10</span><br><span class="line">18/10/22 13:40:47 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10</span><br><span class="line">18/10/22 13:40:47 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: Retry cache on namenode is enabled</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: Computing capacity <span class="keyword">for</span> map NameNodeRetryCache</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: VM <span class="built_in">type</span>       = 64-bit</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB</span><br><span class="line">18/10/22 13:40:47 INFO util.GSet: capacity      = 2^15 = 32768 entries</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSImage: Allocated new BlockPoolId: BP-648750324-127.0.0.1-1540186847111</span><br><span class="line">18/10/22 13:40:47 INFO common.Storage: Storage directory /home/hadoop/hadoop-2.9.1/tmp/dfs/name has been successfully formatted.</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSImageFormatProtobuf: Saving image file /home/hadoop/hadoop-2.9.1/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression</span><br><span class="line">18/10/22 13:40:47 INFO namenode.FSImageFormatProtobuf: Image file /home/hadoop/hadoop-2.9.1/tmp/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved <span class="keyword">in</span> 0 seconds .</span><br><span class="line">18/10/22 13:40:47 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 0</span><br><span class="line">18/10/22 13:40:47 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at localhost.localdomain/127.0.0.1</span><br><span class="line">************************************************************/</span><br><span class="line">[hadoop@localhost hadoop-2.9.1]$ </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@localhost hadoop-2.9.1]$ tree tmp/</span><br><span class="line">tmp/</span><br><span class="line">└── dfs</span><br><span class="line">    └── name</span><br><span class="line">        └── current</span><br><span class="line">            ├── fsimage_0000000000000000000</span><br><span class="line">            ├── fsimage_0000000000000000000.md5</span><br><span class="line">            ├── seen_txid</span><br><span class="line">            └── VERSION</span><br><span class="line"></span><br><span class="line">3 directories, 4 files</span><br><span class="line">[hadoop@localhost hadoop-2.9.1]$ </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>fsimage是NameNode元数据在内存满了后，持久化保存到的文件。</p>
<p>fsimage*.md5 是校验文件，用于校验fsimage的完整性。</p>
<p>seen_txid 是hadoop的版本</p>
<p>vession文件里保存：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop-2.9.1]$ <span class="built_in">cat</span> tmp/dfs/name/current/VERSION </span><br><span class="line"><span class="comment">#Mon Oct 22 13:40:47 CST 2018</span></span><br><span class="line">namespaceID=1544637935</span><br><span class="line">clusterID=CID-85451f7e-c811-4028-8eee-62d3202c00bc</span><br><span class="line">cTime=1540186847111</span><br><span class="line">storageType=NAME_NODE</span><br><span class="line">blockpoolID=BP-648750324-127.0.0.1-1540186847111</span><br><span class="line">layoutVersion=-63</span><br></pre></td></tr></table></figure>
<p>6、 启动NameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop-2.9.1]$ hadoop-daemon.sh start namenode</span><br><span class="line">starting namenode, logging to /home/hadoop/hadoop-2.9.1/logs/hadoop-hadoop-namenode-localhost.localdomain.out</span><br></pre></td></tr></table></figure>
<p>7、 启动DataNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop-2.9.1]$ hadoop-daemon.sh start datanode</span><br><span class="line">starting datanode, logging to /home/hadoop/hadoop-2.9.1/logs/hadoop-hadoop-datanode-localhost.localdomain.out</span><br></pre></td></tr></table></figure>
<p>8、 启动SecondaryNameNode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop-2.9.1]$ hadoop-daemon.sh start secondarynamenode</span><br><span class="line">starting secondarynamenode, logging to /home/hadoop/hadoop-2.9.1/logs/hadoop-hadoop-secondarynamenode-localhost.localdomain.out</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@localhost hadoop-2.9.1]$ jps</span><br><span class="line">7489 NameNode</span><br><span class="line">8167 SecondaryNameNode</span><br><span class="line">8699 Jps</span><br><span class="line">8063 DataNode</span><br><span class="line"></span><br><span class="line">  ├─java,7489 -Dproc_namenode -Xmx1000m -Djava.library.path=/home/hadoop/hadoop-2.9.1/lib -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop-hadoop-namenode-localhost.localdomain.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode</span><br><span class="line">  </span><br><span class="line">  ├─java,8063 -Dproc_datanode -Xmx1000m -Djava.library.path=/home/hadoop/hadoop-2.9.1/lib -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop-hadoop-datanode-localhost.localdomain.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -server -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=ERROR,RFAS -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode</span><br><span class="line"></span><br><span class="line">  ├─java,8167 -Dproc_secondarynamenode -Xmx1000m -Djava.library.path=/home/hadoop/hadoop-2.9.1/lib -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.log.dir=/home/hadoop/hadoop-2.9.1/logs -Dhadoop.log.file=hadoop-hadoop-secondarynamenode-localhost.localdomain.log -Dhadoop.home.dir=/home/hadoop/hadoop-2.9.1 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=<span class="literal">true</span> -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">	[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-<span class="built_in">cat</span> [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-checksum &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">chgrp</span> [-R] GROUP PATH...]</span><br><span class="line">	[-<span class="built_in">chmod</span> [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">	[-<span class="built_in">chown</span> [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">	[-copyFromLocal [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-count [-q] [-h] [-v] [-t [&lt;storage <span class="built_in">type</span>&gt;]] [-u] [-x] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">cp</span> [-f] [-p | -p[topax]] [-d] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">	[-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">	[-<span class="built_in">df</span> [-h] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">du</span> [-s] [-h] [-x] &lt;path&gt; ...]</span><br><span class="line">	[-expunge]</span><br><span class="line">	[-find &lt;path&gt; ... &lt;expression&gt; ...]</span><br><span class="line">	[-get [-f] [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">	[-getfacl [-R] &lt;path&gt;]</span><br><span class="line">	[-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">	[-getmerge [-<span class="built_in">nl</span>] [-skip-empty-file] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">help</span> [cmd ...]]</span><br><span class="line">	[-<span class="built_in">ls</span> [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]]</span><br><span class="line">	[-<span class="built_in">mkdir</span> [-p] &lt;path&gt; ...]</span><br><span class="line">	[-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">	[-<span class="built_in">mv</span> &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-put [-f] [-p] [-l] [-d] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">	[-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;]</span><br><span class="line">	[-<span class="built_in">rm</span> [-f] [-r|-R] [-skipTrash] [-safely] &lt;src&gt; ...]</span><br><span class="line">	[-<span class="built_in">rmdir</span> [--ignore-fail-on-non-empty] &lt;<span class="built_in">dir</span>&gt; ...]</span><br><span class="line">	[-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--<span class="built_in">set</span> &lt;acl_spec&gt; &lt;path&gt;]]</span><br><span class="line">	[-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;]</span><br><span class="line">	[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">stat</span> [format] &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">tail</span> [-f] &lt;file&gt;]</span><br><span class="line">	[-<span class="built_in">test</span> -[defsz] &lt;path&gt;]</span><br><span class="line">	[-text [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">	[-touchz &lt;path&gt; ...]</span><br><span class="line">	[-<span class="built_in">truncate</span> [-w] &lt;length&gt; &lt;path&gt; ...]</span><br><span class="line">	[-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are:</span><br><span class="line">-conf &lt;configuration file&gt;        specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;               define a value <span class="keyword">for</span> a given property</span><br><span class="line">-fs &lt;file:///|hdfs://namenode:port&gt; specify default filesystem URL to use, overrides <span class="string">&#x27;fs.defaultFS&#x27;</span> property from configurations.</span><br><span class="line">-jt &lt;<span class="built_in">local</span>|resourcemanager:port&gt;  specify a ResourceManager</span><br><span class="line">-files &lt;file1,...&gt;                specify a comma-separated list of files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;jar1,...&gt;               specify a comma-separated list of jar files to be included <span class="keyword">in</span> the classpath</span><br><span class="line">-archives &lt;archive1,...&gt;          specify a comma-separated list of archives to be unarchived on the compute machines</span><br><span class="line"></span><br><span class="line">The general <span class="built_in">command</span> line syntax is:</span><br><span class="line"><span class="built_in">command</span> [genericOptions] [commandOptions]</span><br><span class="line"></span><br><span class="line">Usage: hadoop fs [generic options] -<span class="built_in">ls</span> [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [&lt;path&gt; ...]</span><br><span class="line">[hadoop@localhost ~]$ </span><br></pre></td></tr></table></figure>

<p>9、 HDFS上测试创建目录、上传、下载文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个目录，</span></span><br><span class="line">[hadoop@localhost ~]$ hdfs dfs -<span class="built_in">mkdir</span> /demo1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传本地文件到HDFS上</span></span><br><span class="line">[hadoop@localhost ~]$ hdfs dfs -put hadoop-2.9.1.tar.gz  /demo1</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取HDFS上的文件内容</span></span><br><span class="line">[hadoop@localhost ~]$ hdfs dfs -put hadoop-2.9.1/bin/hadoop /demo1</span><br><span class="line">[hadoop@localhost ~]$ hdfs dfs -<span class="built_in">ls</span> -R /</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2018-10-22 18:15 /demo1</span><br><span class="line">-rw-r--r--   1 hadoop supergroup       6656 2018-10-22 18:15 /demo1/hadoop</span><br><span class="line">-rw-r--r--   1 hadoop supergroup  361355307 2018-10-22 18:06 /demo1/hadoop-2.9.1.tar.gz</span><br><span class="line"><span class="comment">#读取HDFS上的文件内容</span></span><br><span class="line">[hadoop@localhost ~]$ hdfs dfs -<span class="built_in">cat</span> /demo1/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从HDFS上下载文件到本地</span></span><br><span class="line">[hadoop@localhost ~]$ hdfs dfs -get /demo1/hadoop</span><br></pre></td></tr></table></figure>

<p>配置、启动YARN</p>
<p>1、 配置mapred-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认没有mapred-site.xml文件，但是有个mapred-site.xml.template配置模板文件。复制模板生成mapred-site.xml。</span></span><br><span class="line">[hadoop@localhost hadoop]$ <span class="built_in">cp</span> mapred-site.xml.template  mapred-site.xml</span><br><span class="line">[hadoop@localhost hadoop]$ vi mapred-site.xml</span><br><span class="line">[hadoop@localhost hadoop]$ <span class="built_in">cat</span> mapred-site.xml</span><br><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span><br><span class="line">&lt;?xml-stylesheet <span class="built_in">type</span>=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the <span class="string">&quot;License&quot;</span>);</span><br><span class="line">  you may not use this file except <span class="keyword">in</span> compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span><br><span class="line">  distributed under the License is distributed on an <span class="string">&quot;AS IS&quot;</span> BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License <span class="keyword">for</span> the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides <span class="keyword">in</span> this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;指定mapreduce运行在yarn框架上。</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">[hadoop@localhost hadoop]$</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>2、 配置yarn-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">添加配置如下：</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;your ip&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>3、 启动Resourcemanager</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop]$ yarn-daemon.sh start resourcemanager</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/hadoop-2.9.1/logs/yarn-hadoop-resourcemanager-localhost.localdomain.out</span><br></pre></td></tr></table></figure>

<p>4、 启动nodemanager</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[hadoop@localhost hadoop]$ yarn-daemon.sh start nodemanager</span><br><span class="line">starting nodemanager, logging to /home/hadoop/hadoop-2.9.1/logs/yarn-hadoop-nodemanager-localhost.localdomain.out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>5、 查看是否启动成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@localhost hadoop]$ jps</span><br><span class="line">7255 ResourceManager</span><br><span class="line">6105 DataNode</span><br><span class="line">5834 NameNode</span><br><span class="line">7532 NodeManager</span><br><span class="line">6412 SecondaryNameNode</span><br><span class="line">7678 Jps  <span class="comment"># 可以看到ResourceManager、NodeManager已经启动成功了。</span></span><br></pre></td></tr></table></figure>
<p>6、 YARN的Web页面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">YARN的Web客户端端口号是8088，</span><br><span class="line"></span><br><span class="line">http://10.0.63.48:8088/cluster</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>




<h1 id="完全分布式安装"><a href="#完全分布式安装" class="headerlink" title="完全分布式安装"></a>完全分布式安装</h1><h1 id="高可用模式安装"><a href="#高可用模式安装" class="headerlink" title="高可用模式安装"></a>高可用模式安装</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://sxt&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/hadoop-2.9.1/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;namenode1:2181,namenode2:2181,namenode3:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sxt&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.sxt&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;namenode1,namenode2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 配置rpc通信接口的 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.sxt.namenode1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;namenode1:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.sxt.namenode2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;namenode2:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.sxt.namenode1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;namenode1:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.sxt.namenode2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;namenode2:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;qjournal://namenode1:8485;namenode2:8485;namenode3:8485/sxt&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.client.failover.proxy.provider.sxt&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/hadoop-2.9.1/journal&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>


</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://magesfc.github.io">mage</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://magesfc.github.io/mage/de5f9aaadaa19a9005fc5140c7198eed7fd8b7e5/">https://magesfc.github.io/mage/de5f9aaadaa19a9005fc5140c7198eed7fd8b7e5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://magesfc.github.io" target="_blank">马哥私房菜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/hadoop/">hadoop</a></div><div class="post_share"><div class="social-share" data-image="https://t.mwm.moe/fj/" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="/null" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/null" target="_blank"><img class="post-qr-code-img" src= "/img/loading.gif" data-lazy-src="/null" alt="支付寶"/></a><div class="post-qr-code-desc">支付寶</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/mage/5fa01cc433ce609529dd958233f788745c01668f/"><img class="prev-cover" src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Codemonkey之编码冒险特技模式1-165关卡</div></div></a></div><div class="next-post pull-right"><a href="/mage/9c016a864ddabbb8f6fb2055b17dd05ebec955f6/"><img class="next-cover" src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Hibernate学习之二级缓存</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">mage</div><div class="author-info__description"> 这里是 马哥 的个人博客 </div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">213</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">228</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">40</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/mamh2021"><i class="fab fa-github"></i><span>GitHub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/mamh2021" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Namenode-%E5%92%8C-Datanode"><span class="toc-number">2.</span> <span class="toc-text">Namenode 和 Datanode</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%89%E8%A3%85hadoop%E5%87%86%E5%A4%87"><span class="toc-number">3.</span> <span class="toc-text">安装hadoop准备</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-number">4.</span> <span class="toc-text">本地模式部署</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85"><span class="toc-number">5.</span> <span class="toc-text">Hadoop伪分布式模式安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85"><span class="toc-number">6.</span> <span class="toc-text">完全分布式安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AB%98%E5%8F%AF%E7%94%A8%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85"><span class="toc-number">7.</span> <span class="toc-text">高可用模式安装</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/mage/145f51d0e22fead4fc7b2a02f114a5766064789d/" title="Docker源码学习之容器的随机的名字怎么来的y"><img src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker源码学习之容器的随机的名字怎么来的y"/></a><div class="content"><a class="title" href="/mage/145f51d0e22fead4fc7b2a02f114a5766064789d/" title="Docker源码学习之容器的随机的名字怎么来的y">Docker源码学习之容器的随机的名字怎么来的y</a><time datetime="2023-09-26T14:32:47.000Z" title="更新于 2023-09-26 22:32:47">2023-09-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/mage/eee219318da0b6a390e2e2d9c27459533c760b0c/" title="Docker源码学习之docker-run命令"><img src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker源码学习之docker-run命令"/></a><div class="content"><a class="title" href="/mage/eee219318da0b6a390e2e2d9c27459533c760b0c/" title="Docker源码学习之docker-run命令">Docker源码学习之docker-run命令</a><time datetime="2023-09-26T14:05:28.000Z" title="更新于 2023-09-26 22:05:28">2023-09-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/mage/d8e5ec1afedee09998d034f01da122815a3d1292/" title="Docker源码学习之docker命令行参数解析流程"><img src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker源码学习之docker命令行参数解析流程"/></a><div class="content"><a class="title" href="/mage/d8e5ec1afedee09998d034f01da122815a3d1292/" title="Docker源码学习之docker命令行参数解析流程">Docker源码学习之docker命令行参数解析流程</a><time datetime="2023-09-24T08:07:37.000Z" title="更新于 2023-09-24 16:07:37">2023-09-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/mage/f6c177061422199002a80ffbf5445a81e9751b46/" title="Docker学习之如何源码编译docker"><img src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Docker学习之如何源码编译docker"/></a><div class="content"><a class="title" href="/mage/f6c177061422199002a80ffbf5445a81e9751b46/" title="Docker学习之如何源码编译docker">Docker学习之如何源码编译docker</a><time datetime="2023-09-24T05:50:00.000Z" title="更新于 2023-09-24 13:50:00">2023-09-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/mage/7b272222e11710ea56798b3ea16707c83ea0138f/" title="Android下的配置管理之道编译高通wlan的构建"><img src= "/img/loading.gif" data-lazy-src="https://t.mwm.moe/fj/" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Android下的配置管理之道编译高通wlan的构建"/></a><div class="content"><a class="title" href="/mage/7b272222e11710ea56798b3ea16707c83ea0138f/" title="Android下的配置管理之道编译高通wlan的构建">Android下的配置管理之道编译高通wlan的构建</a><time datetime="2023-09-22T16:13:06.000Z" title="更新于 2023-09-23 00:13:06">2023-09-23</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://t.mwm.moe/fj/')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By mage</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>